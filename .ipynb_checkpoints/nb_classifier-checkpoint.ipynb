{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNative Bayes (implementarea algoritmului in Python)\\nhttps://www.geeksforgeeks.org/naive-bayes-classifiers/\\n\\nIn primul rand avem nevoie de datele de antrenare, de exemplu:\\n\\nIndex simplu | Temperatura(x1) | Vreme(x2) | Umiditate(x3) | Plec in vacanta(Yj)?\\n    0               Rece          Inorat        Mare                NU\\n    1               Placut        Senin         Normal              DA\\n    2               Cald          Insorit       Mare                NU\\n    3               Cald          Insorit       Mica                DA\\n\\n\\n1) Trebuie sa ne dam seama cate clase sunt si care sunt variatile pt fiecare atribut? \\n    R: sunt doua clase Y1(\\'Da\\') si Y2(\\'Nu\\')\\n        X1 -> Rece, Placut, Cald\\n        X2 -> Inorat, Senin, Insorit\\n        X3-> Mare, Normal, Mic\\n\\n2) Se calculeaza probabilitatea de aparitie a fiecarei clase\\n        P(Y1) = P(\\'DA\\') = 2/4 = 1/2 = 50%\\n        P(Y2) = P(\\'NU\\') = 2/4 = 1/2 = 50%\\n\\n3) Se calculeaza probabilitatile de aparitie pentru fiecare feature(atribut)\\n        P(Rece) = 1/4 = 25%\\n        P(Cald) = 2/4 = 50%\\n        P(Inorat) = 1/4 = 25%\\n        P(Normal) = 1/4 = 25%\\n        ..... si tot asa pt toate\\n\\n4) Se calculeaza probabilitatile conditionate P(X|Y)\\n    P(Rece|NU) = 1/2 = 50% (avem 2 pentru clasa \"nu\" si Rece apare 1 singura data)\\n    P(Rece|Da) = 0% (Rece nu apare nicaieri in clasa \"Da\")\\n    P(Senin|Nu) = 0%\\n    P(senint|Da) = 50%\\n\\n(pana in punctul asta ai realizat procesul de antrenare\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "'''\n",
    "Native Bayes (implementarea algoritmului in Python)\n",
    "https://www.geeksforgeeks.org/naive-bayes-classifiers/\n",
    "\n",
    "In primul rand avem nevoie de datele de antrenare, de exemplu:\n",
    "\n",
    "Index simplu | Temperatura(x1) | Vreme(x2) | Umiditate(x3) | Plec in vacanta(Yj)?\n",
    "    0               Rece          Inorat        Mare                NU\n",
    "    1               Placut        Senin         Normal              DA\n",
    "    2               Cald          Insorit       Mare                NU\n",
    "    3               Cald          Insorit       Mica                DA\n",
    "\n",
    "\n",
    "1) Trebuie sa ne dam seama cate clase sunt si care sunt variatile pt fiecare atribut? \n",
    "    R: sunt doua clase Y1('Da') si Y2('Nu')\n",
    "        X1 -> Rece, Placut, Cald\n",
    "        X2 -> Inorat, Senin, Insorit\n",
    "        X3-> Mare, Normal, Mic\n",
    "\n",
    "2) Se calculeaza probabilitatea de aparitie a fiecarei clase\n",
    "        P(Y1) = P('DA') = 2/4 = 1/2 = 50%\n",
    "        P(Y2) = P('NU') = 2/4 = 1/2 = 50%\n",
    "\n",
    "3) Se calculeaza probabilitatile de aparitie pentru fiecare feature(atribut)\n",
    "        P(Rece) = 1/4 = 25%\n",
    "        P(Cald) = 2/4 = 50%\n",
    "        P(Inorat) = 1/4 = 25%\n",
    "        P(Normal) = 1/4 = 25%\n",
    "        ..... si tot asa pt toate\n",
    "\n",
    "4) Se calculeaza probabilitatile conditionate P(X|Y)\n",
    "    P(Rece|NU) = 1/2 = 50% (avem 2 pentru clasa \"nu\" si Rece apare 1 singura data)\n",
    "    P(Rece|Da) = 0% (Rece nu apare nicaieri in clasa \"Da\")\n",
    "    P(Senin|Nu) = 0%\n",
    "    P(senint|Da) = 50%\n",
    "\n",
    "(pana in punctul asta ai realizat procesul de antrenare\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NBClassifier():\n",
    "    ''' Metoda pentru antrenarea modelului\n",
    "\n",
    "    - aici populam dictionarul in care sctocam informatiile din antrenare(self.nb_dict) si tot aici\n",
    "    se vor defini cateva atribute ale obiectului ce vor fi ultile in procesul de antrenare.\n",
    "\n",
    "    X = numpy-array, contine feature-urile pentru exemplele de antrenare\n",
    "    Y = este o lista ce contine label-urile/etichetele pentru exemplele de antrenare\n",
    "\n",
    "    '''\n",
    "    def train(self, X, y):\n",
    "        self.data = X\n",
    "        self.labels = np.unique(y) #clasele gasite in dataset si le luam pe cele unice\n",
    "        self.class_probabilities = self._calculate_relative_proba(y) #stocheaza prob de aparitie a fiecarei clase { 'Da' = 0.5, 'NU' = 0.5 }\n",
    "        pprint(self.class_probabilities)\n",
    "        self._initialize_nb_dict()  # Creates the dict which stores the information gathered durin training\n",
    "        examples_no, features_no = self.data.shape\n",
    "\n",
    "        '''\n",
    "        Pentru fiecare clasa prezentata in dataset, o sa cream o lista X_class ce o sa contina exemplele de antrenare \n",
    "        pentru clasa respectiva.\n",
    "        \n",
    "        De exemplu: X_Class = [ ['Placut', 'Senin\", 'Normala'], ['Cald', 'Insorit', 'Mica']] --> Pentru clasele 'Da'\n",
    "        X_Class = X = [    ['Rece', 'Innorat', 'Mare'], ['Cald', Insorit, 'Mare']] ---> Pentru clasele 'Nu'\n",
    "        \n",
    "        Mai departe, o să creăm o cheie pentru fiecare feature (x1, x2, ..., xn) în dicționarele ce reprezintă valori \n",
    "        pentru cheile inițiale (”DA” și ”NU”). \n",
    "        La aceste chei ce reprezintă tipul de feature, o să adaugăm în listă toate aparițiile pentru tipul respectiv\n",
    "         de feature ținând cont de clasa\n",
    "        La finalul acestui bloc de cod, self.nb_dict va avea forma:{\n",
    "\n",
    "        'DA': defaultdict(<class 'list'>,\n",
    "        \n",
    "                           {0: ['Placut', 'Cald'],\n",
    "        \n",
    "                            1: ['Senin', 'Insorit'],\n",
    "        \n",
    "                            2: ['Normala', 'Mica']}),\n",
    "        \n",
    "         'NU': defaultdict(<class 'list'>,\n",
    "        \n",
    "                           {0: ['Rece', 'Cald'],\n",
    "        \n",
    "                            1: ['Innorat', 'Insorit'],\n",
    "        \n",
    "                            2: ['Mare', 'Mare']})\n",
    "        \n",
    "        }\n",
    "            \n",
    "            Acum că avem dicționarul ce conține apariția tuturor atributelor/feature-urilor în funcție de clasă și tipul\n",
    "             feature-ului, trebuie să obținem probabilitățile relative ale acestora.\n",
    "    Pentru a realiza acest lucru, o să iterăm prin intregul dicționar și pentru fiecare cheie de feature, o să înlocuim\n",
    "    valoarea acestuia (lista) cu dicționarul returnat de self._calculate_relative_proba .\n",
    "    Forma finală a lui self.nb_dict o să fie:{\n",
    "        'DA': defaultdict(<class 'list'>,\n",
    "        \n",
    "                           {0: {'Cald': 0.5, 'Placut': 0.5},\n",
    "        \n",
    "                            1: {'Insorit': 0.5, 'Senin': 0.5},\n",
    "        \n",
    "                            2: {'Mica': 0.5, 'Normala': 0.5}}),\n",
    "        \n",
    "         'NU': defaultdict(<class 'list'>,\n",
    "        \n",
    "                           {0: {'Cald': 0.5, 'Rece': 0.5},\n",
    "        \n",
    "                            1: {'Innorat': 0.5, 'Insorit': 0.5},\n",
    "        \n",
    "                            2: {'Mare': 1.0}})\n",
    "    \n",
    "    }\n",
    "    Dicționarul de mai sus, este practic, informația invățată de clasificatorul nostru.\n",
    "        '''\n",
    "        for label in self.labels:\n",
    "            X_class = []  # a list which will contain all the examples for a specific class/label\n",
    "            for example_index, example_label in enumerate(y):\n",
    "                if example_label == label:\n",
    "                    X_class.append(X[example_index])\n",
    "\n",
    "            examples_class_no, features_class_no = np.shape(X_class)\n",
    "\n",
    "            for feature_index in range(features_class_no):\n",
    "                for item in X_class:\n",
    "                    self.nb_dict[label][feature_index].append(item[feature_index])\n",
    "\n",
    "        # Now we have a dictionary containing all occurences of feature values, per feature, per class\n",
    "        # We need to transform this dict to a dict with relative feature value probabilities per class\n",
    "        for label in self.labels:\n",
    "            for feature_index in range(features_no):\n",
    "                self.nb_dict[label][feature_index] = self._calculate_relative_proba(self.nb_dict[label][feature_index])\n",
    "\n",
    "\n",
    "\n",
    "    '''\n",
    "        Parametrul X_new este exemplul ce urmează să fie clasificat.\n",
    "    Se crează dicționarul Y_dict pentru a putea păstra probabilitățile de apartenență(scorurile) ale exemplului pentru fiecare clasă.\n",
    "    Se extrag valorile probabilităților condiționate pentru fiecare tip de feature(x1,x2...xn).\n",
    "    Se verifică pentru fiecare feature din X_new dacă are calculată probabilitatea condiționată(dacă există în nb_dict).\n",
    "     În cazul în care feature-ul are deja calculată probabilitatea, se înmulțește valoarea acesteia cu restul probabilităților condiționate ale celorlalte feature-uri și cu probabilitatea pentru clasa (P(y)).\n",
    "    Dacă feature-ul nu are calculată probabilitatea, rezultatul se va înmulți cu zero.\n",
    "     La finalul celui de al-2-lea for, se adaugă valoarea calculată în Y_dict.\n",
    "     În final, se returnează clasa de apartenență a noului exemplu aplând metoda _get_class().\n",
    "    \n",
    "    '''\n",
    "    def predict(self, X_new):\n",
    "        Y_dict = {}\n",
    "\n",
    "        # First we determine the class-probability of each class, and then we determine the class with the highest probability\n",
    "        for label in self.labels:\n",
    "            class_probability = self.class_probabilities[label]\n",
    "\n",
    "            for feature_index in range(len(X_new)):\n",
    "                relative_feature_values = self.nb_dict[label][feature_index]\n",
    "                if X_new[feature_index] in relative_feature_values.keys():\n",
    "                    class_probability *= relative_feature_values[X_new[feature_index]]\n",
    "                else:\n",
    "                    class_probability *= 0.01  # Lidstone smoothing\n",
    "            Y_dict[label] = class_probability\n",
    "\n",
    "        return self._get_class(Y_dict)\n",
    "\n",
    "    '''metoda pentru initializare a dictionarului(de memorare a informatiilor calculate in procesul de antrenare)\n",
    "        np_dict = realizam un dictionar, ce contine la fiecare cheie un default dictionary de tip lista\n",
    "        self.labels-> np.array, elem unice, contine setul de antrenare\n",
    "        #ex: self.labels = ['Da', 'Nu'], iar nb_dict= { 'Da': defaultdict(<list>), 'Nu': defaultdict(<list>) }\n",
    "    '''\n",
    "    def _initialize_nb_dict(self):\n",
    "        self.nb_dict = {}\n",
    "        for label in self.labels:\n",
    "            self.nb_dict[label] = defaultdict(list)\n",
    "\n",
    "    '''metoda pentru calcularea probabilitatilor de aparitie relative/ prob de aparitie \n",
    "        a claselor.\n",
    "        \n",
    "        la input = primeste o lista de feature-uri pt exemplele din setul de antrenare ce apartin\n",
    "        uneia din clasele elements_list['Rece', 'Cald'], feature-urile de Temperatura(x1), pentru clasa 'NU').\n",
    "        - daca dorim sa calculam probabilitatile de aparitie a claselor, lista trb sa contina label-urile pt \n",
    "        exemplele din setul de antrenare.\n",
    "        \n",
    "        Counter = este din \"collecctions\" si ne ajuta sa calculam frecventa de aparitie a elementelor dintr-o lista.\n",
    "        , el ne va intoarce un dictionar unde <key> = elementul din lista, <val> = nr lui de aparitie in lista\n",
    "        \n",
    "        II dai: elements_list = ['Rece', 'Rece\", 'Cald']\n",
    "        si iti scoate 'Rece' : 2/3 % , 'Cald': 1/3 %\n",
    "        P = Cate de 'Rece' este in lista / numarul de lemente\n",
    "        '''\n",
    "    @staticmethod\n",
    "    def _calculate_relative_proba(elements_list):\n",
    "        no_examples = len(elements_list)\n",
    "        occurrence_dict = dict(Counter(elements_list))\n",
    "\n",
    "        for key in occurrence_dict.keys(): #deci pt fiecare cheie, valoarea o sa fie probabilitatea de aparitie\n",
    "            occurrence_dict[key] = occurrence_dict[key] / float(no_examples)\n",
    "\n",
    "        return occurrence_dict\n",
    "    '''\n",
    "    Metoda pentru selectia clasei in functie de scorul cel mai mare\n",
    "        Metoda folosita in metoda predict()\n",
    "        - ne ajuta sa decidem clasa, pe baza probabilitatilor calculate cu Theorema Bayes(clasa cu scorul\n",
    "        cel mai mare, devine clasa exemplului pe care dorim sa il clasificam)\n",
    "        \n",
    "        input: score_dict (este un dictionar ce contine clasele pe post de chei si probabilitatile calculate cu\n",
    "        Bayes pe post de valori.\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    @staticmethod\n",
    "    def _get_class(score_dict):\n",
    "        sorted_dict = sorted(score_dict.items(), key=lambda value: value[1], reverse=True) #sortam descrescator dupa valuare\n",
    "        sorted_dict = dict(sorted_dict)\n",
    "        keys_list = list(sorted_dict.keys())\n",
    "        max_key = keys_list[0]\n",
    "        return max_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
